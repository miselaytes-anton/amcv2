{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miselaytes-anton/amcv2/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tYNzSJv6iTdY",
        "colab_type": "code",
        "outputId": "870d6202-6aa1-4f2f-9139-ca504de632cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "cell_type": "code",
      "source": [
        "import frog"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3fca7f210d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfrog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'frog'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "voxsne2MOS-m",
        "colab_type": "code",
        "outputId": "7f4eb317-b4fb-4027-858b-189cf9fbfb01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.stem.snowball import DutchStemmer \n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "#stemmer = DutchStemmer(ignore_stopwords=True)\n",
        "#wnl = WordNetLemmatizer()\n",
        "\n",
        "bagOfWords = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0HnRpc6DRrq5",
        "colab_type": "code",
        "outputId": "cab04c7d-8ccd-4bc8-b076-a28819a24812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "cell_type": "code",
      "source": [
        "def processText(text):\n",
        "  only_words = re.sub(\"[^a-zA-Z]\", \" \", text)  \n",
        "\n",
        "  words = word_tokenize(only_words, language='dutch')\n",
        "\n",
        "  #   stops = set(stopwords.words(\"dutch\"))                  \n",
        "  #   meaningful_words = [w for w in words if not w in stops] \n",
        "  stems = [stemmer.stem(w) for w in words]\n",
        "  return ' '.join(stems)\n",
        "\n",
        "def printFeatures(features):\n",
        "  vocab = bagOfWords.get_feature_names()\n",
        "  counts = np.sum(features, axis=0).getA()\n",
        "  for tup in zip(vocab, counts[0]):\n",
        "    print(tup)\n",
        "\n",
        "\n",
        "texts = ['Er werd in 2016 een onafhankelijke commissie samengesteld om toe te zien op de ontwikkelingen op het gebied van kunstmatige intelligente. Deze commissie werd eind december 2018 in stilte opgeheven, nadat Google besloot dat de commissie niet efficiënt werkte. Deze commissie kon volgens Google niet goed functioneren en daarom zou er opnieuw worden nagedacht over toezicht op kunstmatige intelligentie ontwikkeld door Google.']\n",
        "processedTexts = [processText(text) for text in texts]\n",
        "features = bagOfWords.fit_transform(processedTexts)\n",
        "printFeatures(features)\n",
        "\n",
        "#tf-idf\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "#both are mainly used either for classification or for finding similarities between documents\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ecc3124e6d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Er werd in 2016 een onafhankelijke commissie samengesteld om toe te zien op de ontwikkelingen op het gebied van kunstmatige intelligente. Deze commissie werd eind december 2018 in stilte opgeheven, nadat Google besloot dat de commissie niet efficiënt werkte. Deze commissie kon volgens Google niet goed functioneren en daarom zou er opnieuw worden nagedacht over toezicht op kunstmatige intelligentie ontwikkeld door Google.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprocessedTexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagOfWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprintFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecc3124e6d88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Er werd in 2016 een onafhankelijke commissie samengesteld om toe te zien op de ontwikkelingen op het gebied van kunstmatige intelligente. Deze commissie werd eind december 2018 in stilte opgeheven, nadat Google besloot dat de commissie niet efficiënt werkte. Deze commissie kon volgens Google niet goed functioneren en daarom zou er opnieuw worden nagedacht over toezicht op kunstmatige intelligentie ontwikkeld door Google.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprocessedTexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagOfWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessedTexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprintFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecc3124e6d88>\u001b[0m in \u001b[0;36mprocessText\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#   stops = set(stopwords.words(\"dutch\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#   meaningful_words = [w for w in words if not w in stops]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mstems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecc3124e6d88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#   stops = set(stopwords.words(\"dutch\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#   meaningful_words = [w for w in words if not w in stops]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mstems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stemmer' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7eHjr9Ib6x2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use:\n",
        "- bag of words features, \n",
        "- or tf-idf features\n",
        "- wordToVec features?\n",
        "\n",
        "\n",
        "to classify the data we get from [here](https://opendata.cbs.nl/ODataApi/OData/83241NED/DiagnosesICPC)\n",
        "So we then train random forest classifyer (see [this tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words) ) or any other classifyer. We use description as input for getting features and we use titles as labels.\n",
        "\n",
        "\n",
        "Also should take a look at [Bert](https://github.com/google-research/bert)\n",
        "\n",
        "https://www.cbs.nl/en-gb/our-services/open-data/website-open-data\n",
        "\n",
        "https://www.nhg.org/themas/artikelen/icpc"
      ]
    },
    {
      "metadata": {
        "id": "lAGwhmg5bgBg",
        "colab_type": "code",
        "outputId": "8666d720-9233-47b7-b7ee-5dcf680c9ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = bagOfWords.get_feature_names()\n",
        "counts = np.sum(features, axis=0).getA()\n",
        "for tup in zip(vocab, counts[0]):\n",
        "  print(tup)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('beslot', 1)\n",
            "('commissie', 4)\n",
            "('daarom', 1)\n",
            "('dat', 1)\n",
            "('de', 2)\n",
            "('december', 1)\n",
            "('deze', 2)\n",
            "('door', 1)\n",
            "('een', 1)\n",
            "('effici', 1)\n",
            "('eind', 1)\n",
            "('en', 1)\n",
            "('er', 2)\n",
            "('functioner', 1)\n",
            "('gebied', 1)\n",
            "('goed', 1)\n",
            "('googl', 3)\n",
            "('het', 1)\n",
            "('in', 2)\n",
            "('intelligent', 1)\n",
            "('intelligentie', 1)\n",
            "('kon', 1)\n",
            "('kunstmat', 2)\n",
            "('nadat', 1)\n",
            "('nagedacht', 1)\n",
            "('niet', 2)\n",
            "('nt', 1)\n",
            "('om', 1)\n",
            "('onafhank', 1)\n",
            "('ontwikkel', 1)\n",
            "('ontwikkeld', 1)\n",
            "('op', 3)\n",
            "('opgehev', 1)\n",
            "('opnieuw', 1)\n",
            "('over', 1)\n",
            "('samengesteld', 1)\n",
            "('stilt', 1)\n",
            "('te', 1)\n",
            "('toe', 1)\n",
            "('toezicht', 1)\n",
            "('van', 1)\n",
            "('volgen', 1)\n",
            "('werd', 2)\n",
            "('werkt', 1)\n",
            "('worden', 1)\n",
            "('zien', 1)\n",
            "('zou', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}